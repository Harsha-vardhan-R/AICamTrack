{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7cedbb5-8675-488a-887b-8297bda1084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # for the super classes and layers\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as func # for activation functions etc..\n",
    "from torch.utils.data import Dataset, DataChunk, DataLoader # for loading data as chunks\n",
    "from torchvision import transforms # image to tensor thing\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchsummary import summary # getting the summary of the architecture\n",
    "\n",
    "import matplotlib\n",
    "import cv2\n",
    "import numpy\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeaaea3-937a-4868-b61f-aa766d005e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to track two things for camera tracking, \n",
    "#    1 -> direction.\n",
    "#    2 -> location.\n",
    "#\n",
    "# The ML model aids us selecting what to track, and because we need to track the camera for more than just the objects that appear at the \n",
    "# beginning of the video, we need to drop objects and pickup new objects with the duration of the clip.\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e48365-e68f-4bd7-a1b6-e1ffe7a88023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "296cc759-5772-474c-b9f0-f7374dc61136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "class SmallShapeLearn(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes = 9):\n",
    "        super(SmallShapeLearn, self).__init__()\n",
    "        # for 200*200 as the input.\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.fc1 = nn.Linear(128 * 25 * 25, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = func.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        #  \\\n",
    "        x = func.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        #   \\\n",
    "        x = func.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        #     \\\n",
    "        x = x.view(-1, 128 * 25 * 25) \n",
    "        x = func.relu(self.fc1(x))\n",
    "        #       \\\n",
    "        x = self.fc2(x)\n",
    "        #        \\\n",
    "        return func.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42d5b73d-f351-464e-a49c-9bf262ce07f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 200, 200]             320\n",
      "         MaxPool2d-2         [-1, 32, 100, 100]               0\n",
      "            Conv2d-3         [-1, 64, 100, 100]          18,496\n",
      "         MaxPool2d-4           [-1, 64, 50, 50]               0\n",
      "            Conv2d-5          [-1, 128, 50, 50]          73,856\n",
      "         MaxPool2d-6          [-1, 128, 25, 25]               0\n",
      "            Linear-7                  [-1, 128]      10,240,128\n",
      "            Linear-8                    [-1, 9]           1,161\n",
      "================================================================\n",
      "Total params: 10,333,961\n",
      "Trainable params: 10,333,961\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.15\n",
      "Forward/backward pass size (MB): 21.36\n",
      "Params size (MB): 39.42\n",
      "Estimated Total Size (MB): 60.94\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# model summary\n",
    "model = SmallShapeLearn(num_classes=9)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "summary(model, (1, 200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc4a01a7-5392-4216-84a0-bc86eb5928ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSet loading\n",
    "shape_label_dictionary = {\"Triangle\" : 0,\n",
    "                          \"Square\" : 1,\n",
    "                          \"Pentagon\" : 2,\n",
    "                          \"Hexagon\" : 3,\n",
    "                          \"Heptagon\" : 4,\n",
    "                          \"Octagon\" : 5,\n",
    "                          \"Nonagon\" : 6,\n",
    "                          \"Circle\" : 7,\n",
    "                          \"Star\" : 8}\n",
    "\n",
    "class ShapeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.root_dir = path\n",
    "        self.image_files = [f for f in os.listdir(path) if f.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        img_name = os.path.join(self.root_dir, self.image_files[id])\n",
    "        image = Image.open(img_name).convert('L')\n",
    "        label = self.image_files[id].split('_')[0]\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((200, 200)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        image = transform(image)\n",
    "        label = shape_label_dictionary[label]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef5f82b-bb3f-425a-8c5e-ca435fe01dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  62%|██████████████████████████████████████████████████████                                 | 1748/2813 [18:48<12:04,  1.47it/s]"
     ]
    }
   ],
   "source": [
    "# Setting up for training\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "model = SmallShapeLearn(num_classes=9)  # Assuming binary classification (Circle or not)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "train_dataset = ShapeDataset('shapes/output')\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c68959-2ced-4362-a51e-c748ba0c0855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f7a1a1-5e19-423c-9736-df314fc2815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PROTOTYPE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04213877-d3fe-4e36-88f2-50262bd9bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERS AND TRACKING #\n",
    "\n",
    "\n",
    "video_path = 'test/footage.mp4'\n",
    "video = cv2.VideoCapture(video_path)\n",
    "kernel = numpy.array([[2 , 1 , 2], \n",
    "                      [1 ,-12 , 1], \n",
    "                      [2 , 1 , 2]])\n",
    "\n",
    "kernel2 = numpy.array([[1, 1 , 2 , 1, 1],\n",
    "                      [1, 3 , 3 , 3, 1], \n",
    "                      [1, 3 ,-44 , 3,1], \n",
    "                      [1, 3 , 3 , 3, 1],\n",
    "                      [1, 1 , 2 , 1, 1]])\n",
    "\n",
    "\n",
    "kernel3 = numpy.array([[-1, -1 , -2, -1, -1],\n",
    "                      [-1, -3 , -3 , -3, -1], \n",
    "                      [-1, -3 , 50 , -3, -1], \n",
    "                      [-1, -3 , -3 , -3, -1],\n",
    "                      [-1, -1 , -2 , -1, -1]])\n",
    "\n",
    "\n",
    "def draw_lines(frame):\n",
    "    \n",
    "    lines = cv2.HoughLines(frame, 1, numpy.pi / 180, threshold=500)\n",
    "\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            rho, theta = line[0]\n",
    "            a = numpy.cos(theta)\n",
    "            b = numpy.sin(theta)\n",
    "            x0 = a * rho\n",
    "            y0 = b * rho\n",
    "            x1 = int(x0 + 1000 * (-b))\n",
    "            y1 = int(y0 + 1000 * (a))\n",
    "            x2 = int(x0 - 1000 * (-b))\n",
    "            y2 = int(y0 - 1000 * (a))\n",
    "            cv2.line(frame, (x1, y1), (x2, y2), (255, 0, 255), 2)\n",
    "\n",
    "\n",
    "if not video.isOpened():\n",
    "    print(\"Error: Could not open video file(mostly, file not found)\")\n",
    "else:\n",
    "    # play video with filters.\n",
    "    while (1):\n",
    "        ret, frame = video.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture frame.\")\n",
    "            break\n",
    "        frame = cv2.resize(frame, (1920, 1080))\n",
    "\n",
    "        frame = cv2.GaussianBlur(frame, (3, 3), 0);\n",
    "        frame = cv2.filter2D(frame, -1, kernel2);\n",
    "        \n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        # if the pressed key in 30 ms is `q`, quit.\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    \n",
    "    video.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcc1f5d-a7cf-4432-a01c-e7aa1ed71984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
